10-3-25 4:50pm Started thinking through how to make a Matrix class work. Has rows and columns, but unsure what the types are for those. Can do multiplication with another Matrix, and can do addition with another Matrix
10-4-25 1:46pm Matrix has a 2d float array called data. Also has a length and depth field (respectively rows and columns). all fields private with getters/setters. A matrix can do multiplication and addition. Working on implementing logic of these methods. Referencing google for how matrix addition/multiiplication work, algorithmically
10-4-25 1:56pm For matrix addition, both matrices must be n*n to be added. if they are, just add element by element into a new matrix. 
10-4-25 2:04pm implemented addition. gonna try to learn how to implement multiplication algorithmically.
10-4-25 2:41pm reference: https://www.geeksforgeeks.org/maths/matrix-multiplication/. For matrix mult. to be defined as operation between two matrices A and B, A must have the same amount of rows as B has columns. The multiplication then returns a new matrix that is ordered by the amount of columns in A, and the amount of rows in B. 
10-4-25 4:49pm stuck on logic for how to make multiplication work so instead just testing addition to make sure its logic is proper. spoiler alert: it is 
10-5-25 4:27pm reference: https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm going to use this for iterative approach to matrix multiplication method. going to alter the algorithm to use the methods and privacy I have already defined. now want to test to make sure this algorithm will be sufficient
10-5-25 5:13pm logic errors in how i am understanding filling a Matrix with numbers. gotta think through how to actually go about filling a matrix with values 
10-5-25 9:04pm fixed logic error in how i was assigning new matrix rows and columns which was causing Arrayoutofboundsexception bc it was not the correct dimensions when trying to fill it. Also fixed logic error when checking dimensions of add(). I was not properly checking whether both dimensions were equal so some m x n's could get thru where m != n. both methods now work. onto sigmoid function 
10-5-25 11:12pm removed unnecessary and wasteful getter calls in class methods. fields are private but we are still in the class so we dont need to use getters
10-6-25 10:14am starting on Main. adding weights, biases, inputs, and output Matrices and filling them with the predetermined values 
10-6-25 2:50pm added hadamard() and transpose() methods. Tranpose makes a new matrix that converts rows from original matrix to columns in new one, and vice versa. hadamard multiples matrices element by element like addition algorithm. logic errors still in how transpose is filling the new matrix (getting AOOB error)
10-8-25 1:12pm fixed logic errors in transpose(). transposing swaps rows and columns so that a matrix that is transposed goes from being an m x n to an n x m, where the rows become the columns and vice cersa. works as intended now and will not go OOB. 
10-9-25 5:04pm beginning work on Main. implementing getActivations() that does a forward pass through a layer and takes weights, old activations, and bias matrices. it works and matches the data in the spreadsheet (up to some decimal place)
10-9-25 6:02pm before beginning core algorithm, need to associate activations, weights and biases like they are associated in all the formulas. i am going to try using associated arrays (implemented as ArrayLists b/c activations will be dynamically calculated as we go). this would also in theory make the getActivations() function more fluid as i am not just passing in hardcoded values but rather GRABBING them as needed 
10-11-25 1:39am forgor to update this. associating indices of minibatches with intended outputs. also intending to reimplement this idea for biases, weights and activations (as they are calculated). forwardPass() and backprop methods work as intended and match the values so the logic is good for them. Now I need to implement the actual epoch -> minibatch -> layer process and understand how to properly do this. 
10-11-25 2:09am WORKING FEED FORWARD -> BACKPROP LOOP!!!! still working through the logic on how to actually perform operations through both minibatches and into 6 epochs
10-11-25 12:17pm noticing very wrong amounts in backprop calculations in one epoch. after x_1, y_1 are fully processed and we move to x_2, y_2 all values are immediately wrong. its definitely got to do with how i am storing the gradients from each step so i will work on a solution to that. the first step does work properly so its not the functions it is my logic i have implemented 
10-11-25 2:32pm starting over with all of my Main logic. minibatching and associating by indices of these batches with their expected outputs. going to implement accumulators for each layer that will make updating easier at the end. These will be matrices of sizes of w_x, b_x (x = 1 or 2) that the gradients are summed with so we dont have to store the gradients outside of each minibatch. FLOW: per epoch -> per minibatch -> forward pass through all layers -> backprop entirely back through -> update weights -> repeat 
10-11-25 3:41pm 1/2 minibatch pass through logic working with iterative approach. now to zoom out and do it per input in the minibatch. 
10-11-25 4:22pm I am retarded. I put 0.25 in w_1 instead of -0.25 and spent a lot of time debugging nothing. my logic worked always and i was being stupid and did not notice that. 
10-11-25 5:09pm error somewhere across weight changing, likely due to when accumulation and updating is happening. first epoch is entirely correct, so i believe there is compounding error somewhere or i am not udnerstanding a specific thing in how we are meant to update weights/biases 
10-11-25 5:34pm error has been fixed and the desired output has been found!!!!! the error came from using my own update function improperly. i was putting arguments in the wrong order and so the original weight was being treated as the accumulator. cleaning up testing prints() and other debugging steps from main loop